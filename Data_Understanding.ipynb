{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undertanding the IMDB Reviews Data Set\n",
    "\n",
    "Instructions\n",
    "* You will need to make a new virtual enviroment so that the libraries we are going to use will work\n",
    "    * Steps to create a virtual enviroment:\n",
    "        * Go to the project directory and run this command:\n",
    "            * python -m venv IMDB_ven\n",
    "        * Then on MacOS, type this command:\n",
    "            * source IMDB_ven/bin/activate\n",
    "        * On Windows, type this command:\n",
    "            * IMDB_ven\\Scripts\\activate\n",
    "        * Now we are in the virtual enviroment\n",
    "* Then download the libraries using the requirements.txt file, we will continue to add to this file if we need more libraries in our project\n",
    "    * Run this command in your terminal to download the libraries:\n",
    "        * pip install -r requirements.txt\n",
    "* Now lets open the aclImdb_v1.tar.gz file\n",
    "    * We can use the tar command to open up the file:\n",
    "        * tar -xzvf aclImdb_v1.tar.gz\n",
    "            * -x: Extracts the files\n",
    "            * -z: Unzips the gzipped file\n",
    "            * -v: Verbosely lists the files being extracted\n",
    "            * -f: Specifies the file name\n",
    "* Word of caution:\n",
    "    * After unzipping the file please do not push the acllmdb file to the github repository since it is very large, that means typing 'git add .' will add this file to the github repository (by basically adding everything) so avoid using the '.'\n",
    "    * Instead just add the files you would like to push by doing 'git add <name_of_file_you_want_to_push>'\n",
    "    * Do not add the aclImdb_v1.tar.gz to github, just have this file in your local enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from wordcloud import WordCloud  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# these libraries are used for the text cleaning\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#these libraries are building the model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer,  BertModel, BertForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# seed for reproducibility\n",
    "seed = 1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a seperate helper function to load reviews from a folder\n",
    "def load_reviews(folder_path, label):\n",
    "    reviews = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            reviews.append(file.read())\n",
    "    return pd.DataFrame({\"review\": reviews, \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will store the base path to the dataset, then load the positive and negative reviews from the training data set to a dataframe\n",
    "base_path = \"aclImdb\"\n",
    "\n",
    "# since this is a classification problem, we will give pos reviews a 1 and neg reviews a 0\n",
    "train_pos = load_reviews(os.path.join(base_path, \"train/pos\"), label=1)\n",
    "train_neg = load_reviews(os.path.join(base_path, \"train/neg\"), label=0)\n",
    "train_data_sorted = pd.concat([train_pos, train_neg]).reset_index(drop=True)\n",
    "train_data = train_data_sorted.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "test_pos = load_reviews('./aclImdb/test/pos', label=1)\n",
    "test_neg = load_reviews('./aclImdb/test/neg', label=0)\n",
    "test_data = pd.concat([test_pos, test_neg]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete later, smaller data for faster testing\n",
    "\n",
    "train_data = train_data.sample(n=5000, random_state=seed)\n",
    "test_data = test_data.sample(n=5000, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the data to get a rough idea of what we have\n",
    "print(\"Number of reviews:\", len(train_data))\n",
    "display(train_data)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of reviews (test):\", len(train_data))\n",
    "display(test_data)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to check the distribution of the data so that we can identify any imbalanced in the data\n",
    "print(\"Label distribution:\\n\", train_data[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like the data is evenly balanced, so now we want to see how long these reviews are\n",
    "train_data[\"review_length\"] = train_data[\"review\"].apply(len) # creating a new column that stores the length of the data\n",
    "plt.hist(train_data[\"review_length\"], bins=30)\n",
    "plt.xlabel(\"Review Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Review Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can try to see the length of reviews for both the negative and positive reviews as well\n",
    "pos_reviews = train_data[train_data[\"label\"] == 1]\n",
    "neg_reviews = train_data[train_data[\"label\"] == 0]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# positive reviews\n",
    "plt.subplot(1, 2, 1) # this graph will be stored in the first row first column\n",
    "plt.hist(pos_reviews[\"review_length\"], bins=30)\n",
    "plt.title(\"Distribution of Positive Review Lengths\")\n",
    "plt.xlabel(\"Review Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# negative reviews\n",
    "plt.subplot(1, 2, 2) # this graph will be stored in the first row second column\n",
    "plt.hist(neg_reviews[\"review_length\"], bins=30)\n",
    "plt.title(\"Distribution of Negative Review Lengths\")\n",
    "plt.xlabel(\"Review Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it looks like the negative reviews have slightly longer reviews compared to the positive reviews, but at a galnce it does not look like much of a difference\n",
    "# now I want to display the word frequency of the review data by using the word cloud, but first I will clean the text data and save it as a column to the data frame\n",
    "\n",
    "# we need to run this bit of code for the cleaning to work, but you only need to run it once\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function clean text function to convert text to lowercase, remove special characters\n",
    "# (punctuation, numbers, etc.), remove stop words, tokenize, and apply lemmatization\n",
    "\n",
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "\n",
    "  text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  stop_words = set(stopwords.words(\"english\"))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "  cleaned_text = ' '.join(tokens)\n",
    "\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned_review\"] = train_data[\"review\"].apply(clean_text) # create a new column in our data frame that has the cleaned text so we can use it later\n",
    "# we wont be using this cleaned text in the BERT model since it would perform poorly on pre cleaned data, so we will just use this for analysis and such\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a look at the word cloud for the positive reviews then the neagtive reviews\n",
    "pos_text = ' '.join(train_data[train_data[\"label\"] == 1][\"cleaned_review\"])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(pos_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Positive Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then the negatvie reviews\n",
    "neg_text = ' '.join(train_data[train_data[\"label\"] == 0][\"cleaned_review\"])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"black\").generate(pos_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Negative Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the changes we made to the datafram again here\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: Positive \n",
      "Text 2: Negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load a fine-tuned model for sentiment analysis\n",
    "model_name = 'textattack/bert-base-uncased-SST-2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text1 = 'This is a good movie. I like it'\n",
    "text2 = 'This is a bad movie. I hate it'\n",
    "\n",
    "# Tokenize and get predictions\n",
    "tokens1 = tokenizer(text1, return_tensors='pt', padding=True, truncation=True)\n",
    "tokens2 = tokenizer(text2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get model predictions\n",
    "result1 = model(**tokens1)\n",
    "result2 = model(**tokens2)\n",
    "\n",
    "# Extract the predicted sentiment class (0 = negative, 1 = positive)\n",
    "sentiment1 = int(torch.argmax(result1.logits))\n",
    "sentiment2 = int(torch.argmax(result2.logits))\n",
    "\n",
    "# Output the sentiment labels\n",
    "sentiment_map = {0: 'Negative', 1: 'Positive'}\n",
    "\n",
    "print(f'Text 1: {sentiment_map[sentiment1]} \\nText 2: {sentiment_map[sentiment2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with cleaned data\n",
    "'''\n",
    "X_train = list(train_data['cleaned_review'])\n",
    "X_test = list(test_data['cleaned_review'])\n",
    "Y_train = list(train_data['label'])\n",
    "Y_test = list(test_data['label'])\n",
    "\n",
    "#splitting the train set into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.2, random_state=seed)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(train_data['review'])\n",
    "X_test = list(test_data['review'])\n",
    "Y_train = list(train_data['label'])\n",
    "Y_test = list(test_data['label'])\n",
    "\n",
    "#splitting the train set into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a BERT tokenizer, reutrns a list of input IDs with the appropriate special tokens.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = tokenizer(X_train, truncation=True, padding=True, max_length=512)\n",
    "X_val_enc = tokenizer(X_val, truncation=True, padding=True, max_length=512)\n",
    "X_test_enc = tokenizer(X_test, truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
