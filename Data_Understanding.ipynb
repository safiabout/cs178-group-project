{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undertanding the IMDB Reviews Data Set\n",
    "\n",
    "Instructions\n",
    "* You will need to make a new virtual enviroment so that the libraries we are going to use will work\n",
    "    * Steps to create a virtual enviroment:\n",
    "        * Go to the project directory and run this command:\n",
    "            * python -m venv IMDB_ven\n",
    "        * Then on MacOS, type this command:\n",
    "            * source IMDB_ven/bin/activate\n",
    "        * On Windows, type this command:\n",
    "            * IMDB_ven\\Scripts\\activate\n",
    "        * Now we are in the virtual enviroment\n",
    "* Then download the libraries using the requirements.txt file, we will continue to add to this file if we need more libraries in our project\n",
    "    * Run this command in your terminal to download the libraries:\n",
    "        * pip install -r requirements.txt\n",
    "* Now lets open the aclImdb_v1.tar.gz file\n",
    "    * We can use the tar command to open up the file:\n",
    "        * tar -xzvf aclImdb_v1.tar.gz\n",
    "            * -x: Extracts the files\n",
    "            * -z: Unzips the gzipped file\n",
    "            * -v: Verbosely lists the files being extracted\n",
    "            * -f: Specifies the file name\n",
    "* Word of caution:\n",
    "    * After unzipping the file please do not push the acllmdb file to the github repository since it is very large, that means typing 'git add .' will add this file to the github repository (by basically adding everything) so avoid using the '.'\n",
    "    * Instead just add the files you would like to push by doing 'git add <name_of_file_you_want_to_push>'\n",
    "    * Do not add the aclImdb_v1.tar.gz to github, just have this file in your local enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Checking if CUDA is enabled\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Understanding the Data #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from wordcloud import WordCloud  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# these libraries are used for the text cleaning\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#these libraries are building the model\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_scheduler\n",
    "\n",
    "# seed for reproducibility\n",
    "seed = 1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a seperate helper function to load reviews from a folder\n",
    "def load_reviews(folder_path, label):\n",
    "    reviews = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            reviews.append(file.read())\n",
    "    return pd.DataFrame({\"review\": reviews, \"label\": label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block for data loading and unloading, only needs to be ran once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# here we will store the base path to the dataset, then load the positive and negative reviews from the training data set to a dataframe\n",
    "base_path = \"aclImdb\"\n",
    "\n",
    "# since this is a classification problem, we will give pos reviews a 1 and neg reviews a 0\n",
    "train_pos = load_reviews(os.path.join(base_path, \"train/pos\"), label=1)\n",
    "train_neg = load_reviews(os.path.join(base_path, \"train/neg\"), label=0)\n",
    "train_data = pd.concat([train_pos, train_neg]).reset_index(drop=True)\n",
    "\n",
    "test_pos = load_reviews('./aclImdb/test/pos', label=1)\n",
    "test_neg = load_reviews('./aclImdb/test/neg', label=0)\n",
    "test_data = pd.concat([test_pos, test_neg]).reset_index(drop=True)\n",
    "\n",
    "full_data = pd.concat([train_data, test_data]).reset_index(drop=True)\n",
    "\n",
    "# save dataframes to a csv files\n",
    "train_data.to_csv(\"train_review_data.csv\", index=False)\n",
    "test_data.to_csv(\"test_review_data.csv\", index=False)\n",
    "full_data.to_csv(\"full_review_data.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes\n",
    "train_data = pd.read_csv(\"train_review_data.csv\",usecols=[\"review\", \"label\"])\n",
    "test_data = pd.read_csv(\"test_review_data.csv\", usecols=[\"review\", \"label\"])\n",
    "full_data = pd.read_csv(\"full_review_data.csv\", usecols=[\"review\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the data to get a rough idea of what we have\n",
    "print(\"Number of reviews:\", len(train_data))\n",
    "display(train_data)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of reviews (test):\", len(train_data))\n",
    "display(test_data)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to check the distribution of the data so that we can identify any imbalanced in the data\n",
    "print(\"Label distribution:\\n\", train_data[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like the data is evenly balanced, so now we want to see how long these reviews are\n",
    "train_data[\"review_length\"] = train_data[\"review\"].apply(len) # creating a new column that stores the length of the data\n",
    "plt.hist(train_data[\"review_length\"], bins=30)\n",
    "plt.xlabel(\"Review Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Review Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can try to see the length of reviews for both the negative and positive reviews as well\n",
    "pos_reviews = train_data[train_data[\"label\"] == 1]\n",
    "neg_reviews = train_data[train_data[\"label\"] == 0]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# positive reviews\n",
    "plt.subplot(1, 2, 1) # this graph will be stored in the first row first column\n",
    "plt.hist(pos_reviews[\"review_length\"], bins=30)\n",
    "plt.title(\"Distribution of Positive Review Lengths\")\n",
    "plt.xlabel(\"Review Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# negative reviews\n",
    "plt.subplot(1, 2, 2) # this graph will be stored in the first row second column\n",
    "plt.hist(neg_reviews[\"review_length\"], bins=30)\n",
    "plt.title(\"Distribution of Negative Review Lengths\")\n",
    "plt.xlabel(\"Review Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it looks like the negative reviews have slightly longer reviews compared to the positive reviews, but at a galnce it does not look like much of a difference\n",
    "# now I want to display the word frequency of the review data by using the word cloud, but first I will clean the text data and save it as a column to the data frame\n",
    "\n",
    "# we need to run this bit of code for the cleaning to work, but you only need to run it once\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis_and_special_chars(text):\n",
    "    # Remove emojis using a regex pattern\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Removes non-ASCII characters (including emojis)\n",
    "    \n",
    "    # Remove any other non-alphanumeric characters (if needed)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Removes any character that is not a letter, number, or space\n",
    "    \n",
    "    return text\n",
    "\n",
    "# helper function clean text function to convert text to lowercase, remove special characters\n",
    "# (punctuation, numbers, etc.), remove stop words, tokenize, and apply lemmatization\n",
    "\n",
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove HTML tags\n",
    "  text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "  # Remove URLs\n",
    "  text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "  text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "  text = remove_emojis_and_special_chars(text)\n",
    "\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  stop_words = set(stopwords.words(\"english\"))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "  cleaned_text = ' '.join(tokens)\n",
    "\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"cleaned_review\"] = train_data[\"review\"].apply(clean_text) # create a new column in our data frame that has the cleaned text so we can use it later\n",
    "# we wont be using this cleaned text in the BERT model since it would perform poorly on pre cleaned data, so we will just use this for analysis and such\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a look at the word cloud for the positive reviews then the neagtive reviews\n",
    "pos_text = ' '.join(train_data[train_data[\"label\"] == 1][\"cleaned_review\"])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(pos_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Positive Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then the negatvie reviews\n",
    "neg_text = ' '.join(train_data[train_data[\"label\"] == 0][\"cleaned_review\"])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"black\").generate(pos_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud for Negative Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the changes we made to the datafram again here\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training the Model and Evaluation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a fine-tuned model for sentiment analysis\n",
    "model_name = 'textattack/bert-base-uncased-SST-2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "text1 = 'This is a good movie. I like it'\n",
    "text2 = 'This is a bad movie. I hate it'\n",
    "\n",
    "# Tokenize and get predictions\n",
    "tokens1 = tokenizer(text1, return_tensors='pt', padding=True, truncation=True)\n",
    "tokens2 = tokenizer(text2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Get model predictions\n",
    "result1 = model(**tokens1)\n",
    "result2 = model(**tokens2)\n",
    "\n",
    "# Extract the predicted sentiment class (0 = negative, 1 = positive)\n",
    "sentiment1 = int(torch.argmax(result1.logits))\n",
    "sentiment2 = int(torch.argmax(result2.logits))\n",
    "\n",
    "# Output the sentiment labels\n",
    "sentiment_map = {0: 'Negative', 1: 'Positive'}\n",
    "\n",
    "print(f'Text 1: {sentiment_map[sentiment1]} \\nText 2: {sentiment_map[sentiment2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with cleaned data\n",
    "'''\n",
    "X_train = list(train_data['cleaned_review'])\n",
    "X_test = list(test_data['cleaned_review'])\n",
    "Y_train = list(train_data['label'])\n",
    "Y_test = list(test_data['label'])\n",
    "\n",
    "#splitting the train set into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.2, random_state=seed)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(train_data['review'])\n",
    "X_test = list(test_data['review'])\n",
    "Y_train = list(train_data['label'])\n",
    "Y_test = list(test_data['label'])\n",
    "\n",
    "#splitting the train set into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a BERT tokenizer, reutrns a list of input IDs with the appropriate special tokens.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizing function\n",
    "def tokenize_data(texts, tokenizer, max_len=512):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",  # Pads to max_len\n",
    "        truncation=True,      # Truncates to max_len\n",
    "        return_tensors=\"pt\"   # Returns PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_encodings = tokenize_data(X_train, tokenizer)\n",
    "val_encodings = tokenize_data(X_val, tokenizer)\n",
    "test_encodings = tokenize_data(X_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = IMDBDataset(train_encodings, Y_train)\n",
    "val_dataset = IMDBDataset(val_encodings, Y_val)\n",
    "test_dataset = IMDBDataset(test_encodings, Y_test)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Added seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "# reference: https://stackoverflow.com/questions/60120043/optimizer-and-scheduler-for-bert-fine-tuning\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Evaluation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, scheduler)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_epoch(model, test_loader, device)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT on cleaned data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis_and_special_chars(text):\n",
    "    # Remove emojis using a regex pattern\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Removes non-ASCII characters (including emojis)\n",
    "    \n",
    "    # Remove any other non-alphanumeric characters (if needed)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Removes any character that is not a letter, number, or space\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    \n",
    "    # Normalize repeated punctuation\n",
    "    text = re.sub(r'([!?])\\1+', r'\\1', text)\n",
    "    \n",
    "    # Fix spacing after punctuation\n",
    "    text = re.sub(r'([.,!?])([^\\s])', r'\\1 \\2', text)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Optionally, convert to lowercase (for bert-base-uncased model)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emojis and special characters\n",
    "    text = remove_emojis_and_special_chars(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of the original dataframes\n",
    "clean_train_data = train_data.copy()\n",
    "clean_test_data = test_data.copy()\n",
    "\n",
    "# Clean the text on both test and train data to see if BERT performs better on it\n",
    "clean_train_data['review'] = clean_train_data['review'].apply(clean_text)\n",
    "clean_test_data['review'] = clean_test_data['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zentropa has much in common with the third man...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zentropa is the most original movie ive seen i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lars von trier is never backward in trying out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>contains spoilers due to me having to describe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that was the first thing that sprang to mind a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>there just isnt enough here there a few funny ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>tainted look at kibbutz lifethis film is less ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>i saw this movie just now not when it was rele...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>any film which begins with a cowhand shagging ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>yes awa wrestling how can anyone forget about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "0      zentropa has much in common with the third man...      1\n",
       "1      zentropa is the most original movie ive seen i...      1\n",
       "2      lars von trier is never backward in trying out...      1\n",
       "3      contains spoilers due to me having to describe...      1\n",
       "4      that was the first thing that sprang to mind a...      1\n",
       "...                                                  ...    ...\n",
       "24995  there just isnt enough here there a few funny ...      0\n",
       "24996  tainted look at kibbutz lifethis film is less ...      0\n",
       "24997  i saw this movie just now not when it was rele...      0\n",
       "24998  any film which begins with a cowhand shagging ...      0\n",
       "24999  yes awa wrestling how can anyone forget about ...      0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(clean_train_data['review'])\n",
    "X_test = list(clean_test_data['review'])\n",
    "Y_train = list(clean_train_data['label'])\n",
    "Y_test = list(clean_test_data['label'])\n",
    "\n",
    "#splitting the train set into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a BERT tokenizer, reutrns a list of input IDs with the appropriate special tokens.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizing function\n",
    "def tokenize_data(texts, tokenizer, max_len=512):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",  # Pads to max_len\n",
    "        truncation=True,      # Truncates to max_len\n",
    "        return_tensors=\"pt\"   # Returns PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_encodings = tokenize_data(X_train, tokenizer)\n",
    "val_encodings = tokenize_data(X_val, tokenizer)\n",
    "test_encodings = tokenize_data(X_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = IMDBDataset(train_encodings, Y_train)\n",
    "val_dataset = IMDBDataset(val_encodings, Y_val)\n",
    "test_dataset = IMDBDataset(test_encodings, Y_test)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Added seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "# reference: https://stackoverflow.com/questions/60120043/optimizer-and-scheduler-for-bert-fine-tuning\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, scheduler)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate_epoch(model, test_loader, device)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
